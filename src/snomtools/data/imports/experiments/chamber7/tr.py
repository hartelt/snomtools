"""Imports time-resolved measurements at Chamber 7."""


from __future__ import absolute_import
from __future__ import division
from __future__ import print_function
import snomtools.data.imports.tiff as tf
import snomtools.data.datasets
import os
import numpy as np
import tifffile
import re
import warnings
import sys
import psutil
import snomtools.calcs.units as u
from pathlib import Path
from snomtools.data.h5tools import probe_chunksize, buffer_needed

if '-v' in sys.argv or __name__ == "__main__":
    verbose = True

else:
    verbose = False

MAX_CACHE_SIZE = None  # 4 * 1024 ** 3  # 4 GB


def ch7_read_tiff_info(filepath):
    """
    Reads tif info files of ch7 to a dict<Info,Valuelist>

    :param filepath: path of info input file
    :return: dict<Info,Valuelist>
    """
    info_re = [
        "(Information.+File) (.+tif)",
        "(Originally.+in) (.+)",
        "(Date): (\d{4}-\d{2}-\d{2})",
        "(Time): (\d{2}:\d{2})",
        "(binning x): (\d+)",
        "(binning y): (\d+)",
        "(binning t): (\d+)",
        "(region of interest x): (\d+) to (\d+)",
        "(region of interest y): (\d+) to (\d+)",
        "(region of interest t): (\d+) to (\d+)",
        "(modulo): (\d+)"
    ]

    info_dict = {}
    print(filepath)

    with open(filepath) as file:
        info = file.read()
        for rexp in info_re:
            groups = re.search(re.compile(rexp), info).groups()
            tmp_value_list = []

            for value in groups[1:]:
                tmp_value_list.append(value)
            info_dict[groups[0]] = tmp_value_list

    return info_dict


def peem_dld_read_ch7(filepath):
    """
    Reads a tif file as generated by ch7 when using the DLD. Therefore, the 3D tif dimensions are interpreted as
    time-channel, x and y.

    :param filepath: String: The (absolute or relative) path of input file.

    :return: The dataset instance generated from the tif file.
    """
    # Translate input path to absolute path:
    filepath = os.path.abspath(filepath)
    filebase = os.path.basename(filepath)
    fileparent = os.path.abspath(os.path.join(filepath, os.pardir))

    # Get Metadata from file_info.txt
    infopath = Path(fileparent , Path(filebase).stem + "_info.txt")
    print(infopath)
    meta_dict = ch7_read_tiff_info(infopath)

    T = meta_dict["region of interest t"][0]
    Tbin = meta_dict["binning t"][0]

    # Read tif file to numpy array. Axes will be (timechannel, x, y):
    infile = tifffile.TiffFile(filepath)
    indata = infile.asarray(np.s_[:])
    infile.close()

    # Initialize data for dataset:
    dataarray = snomtools.data.datasets.DataArray(indata, unit='count', label='counts', plotlabel='Counts')

    # The following commented lines won't work because of Terras irreproducible channel assignment and saving...
    # assert (realdata.shape[0] == round(St / float(Tbin))), \
    # 	"ERROR: Tifffile metadata time binning does not fit to data size."
    # uplim = T+(round(St/float(Tbin)))*Tbin # upper limit calculation
    # So just take the data dimensions... and pray the channels start at the set T value:
    taxis = snomtools.data.datasets.Axis([T + i * Tbin for i in range(indata.shape[0])], label='channel',
                                         plotlabel='Time Channel')


    # Careful about orientation! This is like a matrix:
    # rows go first and are numbered in vertical direction -> Y
    # columns go last and are numbered in horizontal direction -> X
    yaxis = snomtools.data.datasets.Axis(np.arange(0, indata.shape[1]), unit='pixel', label='y', plotlabel='y')
    xaxis = snomtools.data.datasets.Axis(np.arange(0, indata.shape[2]), unit='pixel', label='x', plotlabel='x')

    # Return dataset:
    return snomtools.data.datasets.DataSet(label=filebase, datafields=[dataarray], axes=[taxis, yaxis, xaxis])


def measurement_static_peem(filepath, detector="dld_ch7", scanunit="fs", scanfactor=1,
                            scanaxislabel="scanaxis", scanaxispl=None, h5target=True, chunks=True):
    """
    The base method for importing ch7 static tif

    :param str filepath: The path of the folder containing the scan data. Example: "/path/to/measurement/1. Durchlauf"

    :param str detector: Read mode corresponding to the used detector.
        Valid inputs:
            * :code:`"dld"` for reading the energy-resolved data out of dld tiffs.
            * :code:`"dld-sum"` for reading the sum image out of dld tiffs.
            * :code:`"dld_ch7"` for reading ch7 dld data.
            * :code:`"camera"`

    :param str pattern: The pattern in the filenames that indicates the scan enumeration and is followed by the number
        indicating the position, according to the device used for the scan. Terra uses:
            * :code:`"D"` for a delay stage
            * :code:`"R"` for the rotation mount
            * :code:`"N"` for a dummy device.

    :param str scanunit: A valid unit string, according to the physical dimension that was scanned over.

    :param float scanfactor: A factor that the numbers in the filenames need to be multiplied with to get the
        real value of the scan point in units of scanunit.
        This would be for example:
            * :code:`0.2` (because of stage position) with delayunit :code:`"um"` for normal Interferometer because \
            one decimal is in filenames.
            * :code:`0.2` (because of stage position and strange factor 10 in filenames) with delayunit :code:`"as"` \
            for PR interferometer

    :param str scanaxislabel: A label for the axis of the scan.

    :param str scanaxispl: A plot label for the axis of the scan.

    :param h5target: The HDF5 target to write to.
    :type h5target: str **or** h5py.Group **or** True (the default), *optional*

    .. warning::
        The default value `True` for `h5target` reads into a DataSet in temp file mode.
        The cache size (`chunk_cache_mem_size`) is not forwarded to DataArrays in this DataSet.
        Therefore the data is read in with default cache size, which can be **very** slow!

        Use the proper target for `h5target` directly for maximum performance.

    :param chunks: If given as a tuple of ints of length of the to-be-read data dimensionality, this can be used to
        explicitly set the chunk alignment for the output data. Useful if a specific chunk size is desired.
        If `True` is given (the default), the chunk alignment is chosen automatically.
        Ignored if h5target is `False`.
        Chunking can be turned off by giving `False` (not recommended).
    :type chunks: tuple *of* ints **or** bool

    :return: Imported DataSet.
    :rtype: DataSet
    """
    assert detector in ["dld", "dld-sum", "camera", "dld_ch7"], "Invalid detector mode."
    if scanaxispl is None:
        scanaxispl = 'Scan / ' + scanunit


    # Translate input path to absolute path:
    filepath = os.path.abspath(filepath)

    # Inspect the given folder for time step files:
    scanfiles = {}
    scanfiles[float(0)] = filepath

    # Generate delay axis:
    axlist = []
    for scanstep in iter(sorted(scanfiles.keys())):
        axlist.append(scanstep)
    scanvalues = u.to_ureg(np.array(axlist), scanunit)
    scanvalues = u.to_ureg(np.array(axlist) * scanfactor, scanunit)
    scanaxis = snomtools.data.datasets.Axis(scanvalues, label=scanaxislabel, plotlabel=scanaxispl)

    # Test data size:
    if detector == "dld":
        sample_data = tf.peem_dld_read_terra(os.path.join(filepath, scanfiles[list(scanfiles.keys())[0]]))
    elif detector == "dld-sum":
        sample_data = tf.peem_dld_read_terra_sumimage(os.path.join(filepath, scanfiles[list(scanfiles.keys())[0]]))
    elif detector == "dld_ch7":
        sample_data = peem_dld_read_ch7(os.path.join(filepath, scanfiles[list(scanfiles.keys())[0]]))
    else:
        sample_data = tf.peem_camera_read_terra(os.path.join(filepath, scanfiles[list(scanfiles.keys())[0]]))
    axlist = [scanaxis] + sample_data.axes
    newshape = scanaxis.shape + sample_data.shape

    # Build the data-structure that the loaded data gets filled into
    if h5target:
        compression = 'gzip'
        compression_opts = 4

        # Handle chunking and optimize buffer size:
        if chunks is True:  # Default is auto chunk alignment, so we need to probe.
            max_available_cache = psutil.virtual_memory().available * 0.7  # 70 % of available memory
            if MAX_CACHE_SIZE:
                max_available_cache = min(max_available_cache, MAX_CACHE_SIZE)  # Stay below hardcoded debug limit.
            use_chunk_size = probe_chunksize(shape=newshape, compression=compression, compression_opts=compression_opts)
            use_cache_size = buffer_needed(newshape, (0,), use_chunk_size, dtype=np.uint16)
            while use_cache_size > max_available_cache:
                if verbose:
                    print("Warning: Chunk alignment {0} to large for available cache.".format(use_chunk_size))
                # Reduce chunk scan chunk size by half:
                use_chunk_size = (use_chunk_size[0] // 2,) + use_chunk_size[1:]
                # Note: It looks tempting to do something like this to keep overall chunk size constant:
                # ndims = len(use_chunk_size[1:])
                # use_chunk_size = (use_chunk_size[0] // 2,) + tuple(
                #     int(n * (2 ** (1 / ndims))) for n in use_chunk_size[1:])
                # This is NOT better, because of overhang (length % chunksize) which can exceed buffer!
                # I'll not do an advanced search for optimal values for constant size,
                # because the chunk size should still be reasonable down to 1/16 MB or so.
                # MH, 2020-04-16
                if use_chunk_size[0] < 1:  # Avoid edge case 0
                    use_chunk_size = (1,) + use_chunk_size[1:]
                    print("Warning: Cannot reduce chunk size to fit into available buffer. Readin might be slow!")
                    use_cache_size = buffer_needed(newshape, (0,), use_chunk_size, dtype=np.uint16)
                    break
                if verbose:
                    print("Using half chunk size along scan direction: {0}".format(use_chunk_size))
                use_cache_size = buffer_needed(newshape, (0,), use_chunk_size, dtype=np.uint16)
        elif chunks:  # Chunk size is explicitly set:
            use_chunk_size = chunks
            use_cache_size = buffer_needed(newshape, (0,), use_chunk_size, dtype=np.uint16)
        else:  # Chunked storage is turned off:
            use_chunk_size = False
            use_cache_size = None

        # Initialize full DataSet with zeroes:
        dataspace = snomtools.data.datasets.Data_Handler_H5(unit=sample_data.get_datafield(0).get_unit(),
                                                            shape=newshape, chunks=use_chunk_size,
                                                            compression=compression, compression_opts=compression_opts,
                                                            chunk_cache_mem_size=use_cache_size,
                                                            dtype=np.uint16)
        print(dataspace)
        dataarray = snomtools.data.datasets.DataArray(dataspace,
                                                      label=sample_data.get_datafield(0).get_label(),
                                                      plotlabel=sample_data.get_datafield(0).get_plotlabel(),
                                                      h5target=dataspace.h5target,
                                                      chunks=use_chunk_size,
                                                      compression=compression, compression_opts=compression_opts,
                                                      chunk_cache_mem_size=use_cache_size)
        print(dataarray)
        dataset = snomtools.data.datasets.DataSet("Terra Scan " + filepath, [dataarray], axlist, h5target=h5target,
                                                  chunk_cache_mem_size=use_cache_size)
    else:
        # In-memory data processing without h5 files.
        dataspace = u.to_ureg(np.zeros(newshape, dtype=np.uint16), sample_data.datafields[0].get_unit())
        dataarray = snomtools.data.datasets.DataArray(dataspace,
                                                      label=sample_data.get_datafield(0).get_label(),
                                                      plotlabel=sample_data.get_datafield(0).get_plotlabel(),
                                                      h5target=None)
        dataset = snomtools.data.datasets.DataSet("Terra Scan " + filepath, [dataarray], axlist, h5target=h5target)

    dataarray = dataset.get_datafield(0)

    # Fill in data from imported tiffs:
    slicebase = tuple([np.s_[:] for j in range(len(sample_data.shape))])

    if verbose:
        import time
        print("Reading Terra Scan Folder of shape: ", dataset.shape)
        if h5target:
            print("... generating chunks of shape: ", dataset.get_datafield(0).data.ds_data.chunks)
            if dataset.own_h5file:
                print("... using cache size {0:d} MB".format(dataset.h5target.get_chunk_cache_mem_size() // 1024 ** 2))
        else:
            print("... in memory")
        start_time = time.time()

    for i, scanstep in zip(list(range(len(scanfiles))), iter(sorted(scanfiles.keys()))):
        islice = (i,) + slicebase
        # Import tiff:
        if detector == "dld":
            idata = tf.peem_dld_read_terra(os.path.join(filepath, scanfiles[scanstep]))
        elif detector == "dld-sum":
            idata = tf.peem_dld_read_terra_sumimage(os.path.join(filepath, scanfiles[scanstep]))
        elif detector == "dld_ch7":
            sample_data = peem_dld_read_ch7(os.path.join(filepath, scanfiles[scanstep]))
        else:
            idata = tf.peem_camera_read_terra(os.path.join(filepath, scanfiles[scanstep]))
        # Check data consistency:
        assert idata.shape == sample_data.shape, "Trying to combine scan data with different shape."
        for ax1, ax2 in zip(idata.axes, sample_data.axes):
            assert ax1.units == ax2.units, "Trying to combine scan data with different axis dimensionality."
        assert idata.get_datafield(0).units == sample_data.get_datafield(0).units, \
            "Trying to combine scan data with different data dimensionality."
        # Write data:
        dataarray[islice] = idata.get_datafield(0).data
        if verbose:
            tpf = ((time.time() - start_time) / float(i + 1))
            etr = tpf * (dataset.shape[0] - i + 1)
            print("tiff {0:d} / {1:d}, Time/File {3:.2f}s ETR: {2:.1f}s".format(i, dataset.shape[0], etr, tpf))

    return dataset

def measurement_folder_peem(folderpath, detector="dld", pattern="D", scanunit="um", scanfactor=1,
                                  scanaxislabel="scanaxis", scanaxispl=None, h5target=True, chunks=True):
    """
    The base method for importing terra scan folders. Covers all scan possibilities, so far only in 1D scans.

    :param str folderpath: The path of the folder containing the scan data. Example: "/path/to/measurement/1. Durchlauf"

    :param str detector: Read mode corresponding to the used detector.
        Valid inputs:
            * :code:`"dld"` for reading the energy-resolved data out of dld tiffs.
            * :code:`"dld-sum"` for reading the sum image out of dld tiffs.
            * :code:`"dld_ch7"` for reading ch7 dld data.
            * :code:`"camera"`

    :param str pattern: The pattern in the filenames that indicates the scan enumeration and is followed by the number
        indicating the position, according to the device used for the scan. Terra uses:
            * :code:`"D"` for a delay stage
            * :code:`"R"` for the rotation mount
            * :code:`"N"` for a dummy device.
            * :code:`"ch7tr"' for ch7 pattern

    :param str scanunit: A valid unit string, according to the physical dimension that was scanned over.

    :param float scanfactor: A factor that the numbers in the filenames need to be multiplied with to get the
        real value of the scan point in units of scanunit.
        This would be for example:
            * :code:`0.2` (because of stage position) with delayunit :code:`"um"` for normal Interferometer because \
            one decimal is in filenames.
            * :code:`0.2` (because of stage position and strange factor 10 in filenames) with delayunit :code:`"as"` \
            for PR interferometer

    :param str scanaxislabel: A label for the axis of the scan.

    :param str scanaxispl: A plot label for the axis of the scan.

    :param h5target: The HDF5 target to write to.
    :type h5target: str **or** h5py.Group **or** True (the default), *optional*

    .. warning::
        The default value `True` for `h5target` reads into a DataSet in temp file mode.
        The cache size (`chunk_cache_mem_size`) is not forwarded to DataArrays in this DataSet.
        Therefore the data is read in with default cache size, which can be **very** slow!

        Use the proper target for `h5target` directly for maximum performance.

    :param chunks: If given as a tuple of ints of length of the to-be-read data dimensionality, this can be used to
        explicitly set the chunk alignment for the output data. Useful if a specific chunk size is desired.
        If `True` is given (the default), the chunk alignment is chosen automatically.
        Ignored if h5target is `False`.
        Chunking can be turned off by giving `False` (not recommended).
    :type chunks: tuple *of* ints **or** bool

    :return: Imported DataSet.
    :rtype: DataSet
    """
    assert detector in ["dld", "dld-sum", "camera", "dld_ch7"], "Invalid detector mode."
    if scanaxispl is None:
        scanaxispl = 'Scan / ' + scanunit

    # Compile regex for file detection:

    if pattern == "ch7tr":
        pat = re.compile("\d{3}_\d{4}_(.+)_.+tif")
    else:
        pat = re.compile("(-?\d*).tif")

    # Translate input path to absolute path:
    folderpath = os.path.abspath(folderpath)

    # Inspect the given folder for time step files:
    scanfiles = {}
    for filename in filter(tf.is_tif, os.listdir(folderpath)):
        found = re.search(pat, filename)
        if found:
            scanstep = float(found.group(1))
            scanfiles[scanstep] = filename

    # Generate delay axis:
    axlist = []
    for scanstep in iter(sorted(scanfiles.keys())):
        axlist.append(scanstep)
    if pattern == "ch7tr":
        scanvalues = u.to_ureg(np.array(axlist), scanunit)
    else:
        scanvalues = u.to_ureg(np.array(axlist) * scanfactor, scanunit)
    scanaxis = snomtools.data.datasets.Axis(scanvalues, label=scanaxislabel, plotlabel=scanaxispl)

    # Test data size:
    if detector == "dld":
        sample_data = tf.peem_dld_read_terra(os.path.join(folderpath, scanfiles[list(scanfiles.keys())[0]]))
    elif detector == "dld-sum":
        sample_data = tf.peem_dld_read_terra_sumimage(os.path.join(folderpath, scanfiles[list(scanfiles.keys())[0]]))
    elif detector == "dld_ch7":
        sample_data = peem_dld_read_ch7(os.path.join(folderpath, scanfiles[list(scanfiles.keys())[0]]))
    else:
        sample_data = tf.peem_camera_read_terra(os.path.join(folderpath, scanfiles[list(scanfiles.keys())[0]]))
    axlist = [scanaxis] + sample_data.axes
    newshape = scanaxis.shape + sample_data.shape

    # Build the data-structure that the loaded data gets filled into
    if h5target:
        compression = 'gzip'
        compression_opts = 4

        # Handle chunking and optimize buffer size:
        if chunks is True:  # Default is auto chunk alignment, so we need to probe.
            max_available_cache = psutil.virtual_memory().available * 0.7  # 70 % of available memory
            if MAX_CACHE_SIZE:
                max_available_cache = min(max_available_cache, MAX_CACHE_SIZE)  # Stay below hardcoded debug limit.
            use_chunk_size = probe_chunksize(shape=newshape, compression=compression, compression_opts=compression_opts)
            use_cache_size = buffer_needed(newshape, (0,), use_chunk_size, dtype=np.uint16)
            while use_cache_size > max_available_cache:
                if verbose:
                    print("Warning: Chunk alignment {0} to large for available cache.".format(use_chunk_size))
                # Reduce chunk scan chunk size by half:
                use_chunk_size = (use_chunk_size[0] // 2,) + use_chunk_size[1:]
                # Note: It looks tempting to do something like this to keep overall chunk size constant:
                # ndims = len(use_chunk_size[1:])
                # use_chunk_size = (use_chunk_size[0] // 2,) + tuple(
                #     int(n * (2 ** (1 / ndims))) for n in use_chunk_size[1:])
                # This is NOT better, because of overhang (length % chunksize) which can exceed buffer!
                # I'll not do an advanced search for optimal values for constant size,
                # because the chunk size should still be reasonable down to 1/16 MB or so.
                # MH, 2020-04-16
                if use_chunk_size[0] < 1:  # Avoid edge case 0
                    use_chunk_size = (1,) + use_chunk_size[1:]
                    print("Warning: Cannot reduce chunk size to fit into available buffer. Readin might be slow!")
                    use_cache_size = buffer_needed(newshape, (0,), use_chunk_size, dtype=np.uint16)
                    break
                if verbose:
                    print("Using half chunk size along scan direction: {0}".format(use_chunk_size))
                use_cache_size = buffer_needed(newshape, (0,), use_chunk_size, dtype=np.uint16)
        elif chunks:  # Chunk size is explicitly set:
            use_chunk_size = chunks
            use_cache_size = buffer_needed(newshape, (0,), use_chunk_size, dtype=np.uint16)
        else:  # Chunked storage is turned off:
            use_chunk_size = False
            use_cache_size = None

        # Initialize full DataSet with zeroes:
        dataspace = snomtools.data.datasets.Data_Handler_H5(unit=sample_data.get_datafield(0).get_unit(),
                                                            shape=newshape, chunks=use_chunk_size,
                                                            compression=compression, compression_opts=compression_opts,
                                                            chunk_cache_mem_size=use_cache_size,
                                                            dtype=np.uint16)
        dataarray = snomtools.data.datasets.DataArray(dataspace,
                                                      label=sample_data.get_datafield(0).get_label(),
                                                      plotlabel=sample_data.get_datafield(0).get_plotlabel(),
                                                      h5target=dataspace.h5target,
                                                      chunks=use_chunk_size,
                                                      compression=compression, compression_opts=compression_opts,
                                                      chunk_cache_mem_size=use_cache_size)
        dataset = snomtools.data.datasets.DataSet("Terra Scan " + folderpath, [dataarray], axlist, h5target=h5target,
                                                  chunk_cache_mem_size=use_cache_size)
    else:
        # In-memory data processing without h5 files.
        dataspace = u.to_ureg(np.zeros(newshape, dtype=np.uint16), sample_data.datafields[0].get_unit())
        dataarray = snomtools.data.datasets.DataArray(dataspace,
                                                      label=sample_data.get_datafield(0).get_label(),
                                                      plotlabel=sample_data.get_datafield(0).get_plotlabel(),
                                                      h5target=None)
        dataset = snomtools.data.datasets.DataSet("Terra Scan " + folderpath, [dataarray], axlist, h5target=h5target)

    dataarray = dataset.get_datafield(0)

    # Fill in data from imported tiffs:
    slicebase = tuple([np.s_[:] for j in range(len(sample_data.shape))])

    if verbose:
        import time
        print("Reading Terra Scan Folder of shape: ", dataset.shape)
        if h5target:
            print("... generating chunks of shape: ", dataset.get_datafield(0).data.ds_data.chunks)
            if dataset.own_h5file:
                print("... using cache size {0:d} MB".format(dataset.h5target.get_chunk_cache_mem_size() // 1024 ** 2))
        else:
            print("... in memory")
        start_time = time.time()

    for i, scanstep in zip(list(range(len(scanfiles))), iter(sorted(scanfiles.keys()))):
        islice = (i,) + slicebase
        # Import tiff:
        if detector == "dld":
            idata = tf.peem_dld_read_terra(os.path.join(folderpath, scanfiles[scanstep]))
        elif detector == "dld-sum":
            idata = tf.peem_dld_read_terra_sumimage(os.path.join(folderpath, scanfiles[scanstep]))
        elif detector == "dld_ch7":
            sample_data = peem_dld_read_ch7(os.path.join(folderpath, scanfiles[scanstep]))
        else:
            idata = tf.peem_camera_read_terra(os.path.join(folderpath, scanfiles[scanstep]))
        # Check data consistency:
        assert idata.shape == sample_data.shape, "Trying to combine scan data with different shape."
        for ax1, ax2 in zip(idata.axes, sample_data.axes):
            assert ax1.units == ax2.units, "Trying to combine scan data with different axis dimensionality."
        assert idata.get_datafield(0).units == sample_data.get_datafield(0).units, \
            "Trying to combine scan data with different data dimensionality."
        # Write data:
        dataarray[islice] = idata.get_datafield(0).data
        if verbose:
            tpf = ((time.time() - start_time) / float(i + 1))
            etr = tpf * (dataset.shape[0] - i + 1)
            print("tiff {0:d} / {1:d}, Time/File {3:.2f}s ETR: {2:.1f}s".format(i, dataset.shape[0], etr, tpf))

    return dataset
