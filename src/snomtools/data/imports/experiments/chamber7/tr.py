"""Imports time-resolved measurements at Chamber 7."""


from __future__ import absolute_import
from __future__ import division
from __future__ import print_function
import snomtools.data.imports.tiff as tf
import snomtools.data.datasets
import os
import numpy as np
import tifffile
import re
import sys
import psutil
import snomtools.calcs.units as u
from pathlib import Path
from snomtools.data.h5tools import probe_chunksize, buffer_needed

if '-v' in sys.argv or __name__ == "__main__":
    verbose = True
else:
    verbose = False

MAX_CACHE_SIZE = None  # 4 * 1024 ** 3  # 4 GB


def strs_in_path(path, strings):
    for string in strings:
        if string in path:
            return True
    return False


def gen_all_files_of_pathtree(folderpath, filesuffixes, ignores = []):
    """
    Returns a generator that yields the Path to all files with types passed in filesuffixes recursively

    :param str/path folderpath: The folderpath that will be crawled for files of a type

    :param List[str] a list of filesuffixes that are valid. Works only for format ".type" not "type"

    :param List[str] if str occurs in path, path will be ignored.

    :return: Generator for all filepaths found

    ToDo: Testing!
    """

    return (Path(path, file) for path, _, files in os.walk(folderpath) if not strs_in_path(path, ignores) for file in
            files if Path(file).suffix in filesuffixes)


def gen_all_tiff_of_pathtree(folderpath):
    """
    Returns a generator that yields the Path to all .tiff/.tif of a path recursively.

    :param str/path folderpath: The folderpath that will be crawled for files of a type

    :return: Generator for all filepaths found

    Todo: Testing!
    """
    return gen_all_files_of_pathtree(folderpath, [".tiff", ".tif"])


def gen_all_non_tr_tiff(folderpath):
    """
    Returns a generator that yields the Path to all .tiff/.tif of a path recursively and ignores tr folders

    :param str/path folderpath: The folderpath that will be crawled for files of a type

    :return: Generator for all filepaths found

    Todo: Testing!
    """

    return gen_all_files_of_pathtree(folderpath, [".tiff", ".tif"], ignores=["tr", "TR", "Tr"])


def ch7_read_tiff_info(filepath):
    """
    Reads tif info files of ch7 to a dict<Info,Valuelist>

    :param filepath: path of info input file
    :return: dict<Info,Valuelist>
    """
    info_re = [
        "(Information.+File) (.+tif)",
        "(Originally.+in) (.+)",
        "(Date): (\d{4}-\d{2}-\d{2})",
        "(Time): (\d{2}:\d{2})",
        "(binning x): (\d+)",
        "(binning y): (\d+)",
        "(binning t): (\d+)",
        "(region of interest x): (\d+) to (\d+)",
        "(region of interest y): (\d+) to (\d+)",
        "(region of interest t): (\d+) to (\d+)",
        "(modulo): (\d+)"
    ]

    print(filepath)
    info_dict = {}

    with open(filepath) as file:
        info = file.read()
        for rexp in info_re:
            groups = re.search(re.compile(rexp), info).groups()
            tmp_value_list = []

            for value in groups[1:]:
                tmp_value_list.append(value)
            info_dict[groups[0]] = tmp_value_list

    return info_dict


def peem_dld_read_ch7(filepath):
    """
    Reads a tif file as generated by ch7 when using the DLD. Therefore, the 3D tif dimensions are interpreted as
    time-channel, x and y.

    :param filepath: String: The (absolute or relative) path of input file.

    :return: The dataset instance generated from the tif file.
    """
    # Translate input path to absolute path:
    filepath = os.path.abspath(filepath)
    filebase = os.path.basename(filepath)
    fileparent = os.path.abspath(os.path.join(filepath, os.pardir))

    # Get Metadata from file_info.txt
    infopath = Path(fileparent , Path(filebase).stem + "_info.txt")
    print(infopath)
    meta_dict = ch7_read_tiff_info(infopath)

    T = int(meta_dict["region of interest t"][0])
    Xbin = int(meta_dict["binning x"][0])
    Ybin = int(meta_dict["binning y"][0])
    Tbin = int(meta_dict["binning t"][0])

    # Read tif file to numpy array. Axes will be (timechannel, x, y):
    infile = tifffile.TiffFile(filepath)
    indata = infile.asarray(np.s_[:])
    infile.close()

    # Initialize data for dataset:
    dataarray = snomtools.data.datasets.DataArray(indata, unit='count', label='counts', plotlabel='Counts')

    # The following commented lines won't work because of Terras irreproducible channel assignment and saving...
    # assert (realdata.shape[0] == round(St / float(Tbin))), \
    # 	"ERROR: Tifffile metadata time binning does not fit to data size."
    # uplim = T+(round(St/float(Tbin)))*Tbin # upper limit calculation
    # So just take the data dimensions... and pray the channels start at the set T value:
    taxis = snomtools.data.datasets.Axis([T + i * Tbin for i in range(indata.shape[0])], label='channel',
                                         plotlabel='Time Channel')


    # Careful about orientation! This is like a matrix:
    # rows go first and are numbered in vertical direction -> Y
    # columns go last and are numbered in horizontal direction -> X
    yaxis = snomtools.data.datasets.Axis([i * Xbin for i in range(indata.shape[1])],
                                         unit='pixel', label='y', plotlabel='y')
    xaxis = snomtools.data.datasets.Axis([i * Xbin for i in range(indata.shape[2])],
                                         unit='pixel', label='x', plotlabel='x')

    # Return dataset:
    return snomtools.data.datasets.DataSet(label=filebase, datafields=[dataarray], axes=[taxis, yaxis, xaxis])


def peem_dld_read_all_static_ch7(folderpath):
    """
    Greedy function that converts all tiffs anywhere in the folderpath (subdirs too!) use with care! Get a coffee!
    Can produce chaos and despair if it breaks midways.
    IGNORES FOLDERS WITH "tr","Tr","TR" IN THEIR PATH! If that happens you probably want to use measurement_folder_peem

    :param folderpath: Folderpath in which tiff are searched and converted

    :return: None
    """

    for filepath in gen_all_non_tr_tiff(folderpath):
        dest = Path(filepath.parent) / (filepath.stem + ".hdf5")
        print(dest)
        dataset = peem_dld_read_ch7(filepath)
        dataset.saveh5(h5dest=str(dest))


def measurement_folder_peem(folderpath, detector="dld_ch7", pattern="ch7tr", scanunit="fs",
                                  scanaxislabel="delay", scanaxispl=None, h5target=True, chunks=True):
    """
    The base method for importing terra scan folders. Covers all scan possibilities, so far only in 1D scans.

    :param str folderpath: The path of the folder containing the scan data. Example: "/path/to/measurement/1. Durchlauf"

    :param str detector: Read mode corresponding to the used detector.
        Valid inputs:
            * :code:`"dld_ch7"` for reading ch7 dld data.

    :param str pattern: The pattern in the filenames that indicates the scan enumeration and is followed by the number
        indicating the position, according to the device used for the scan. Terra uses:
            * :code:`"ch7tr"' for ch7 pattern

    :param str scanunit: A valid unit string, according to the physical dimension that was scanned over.

    :param str scanaxislabel: A label for the axis of the scan.

    :param str scanaxispl: A plot label for the axis of the scan.

    :param h5target: The HDF5 target to write to.
    :type h5target: str **or** h5py.Group **or** True (the default), *optional*

    .. warning::
        The default value `True` for `h5target` reads into a DataSet in temp file mode.
        The cache size (`chunk_cache_mem_size`) is not forwarded to DataArrays in this DataSet.
        Therefore the data is read in with default cache size, which can be **very** slow!

        Use the proper target for `h5target` directly for maximum performance.

    :param chunks: If given as a tuple of ints of length of the to-be-read data dimensionality, this can be used to
        explicitly set the chunk alignment for the output data. Useful if a specific chunk size is desired.
        If `True` is given (the default), the chunk alignment is chosen automatically.
        Ignored if h5target is `False`.
        Chunking can be turned off by giving `False` (not recommended).
    :type chunks: tuple *of* ints **or** bool

    :return: Imported DataSet.
    :rtype: DataSet

    ToDo: Testing!
    """
    assert detector in ["dld_ch7"], "Invalid detector mode."
    if scanaxispl is None:
        scanaxispl = 'Scan / ' + scanunit

    # Compile regex for file detection:

    if pattern == "ch7tr":
        pat = re.compile("\d{3}_\d{4}_(.+?)_.+tif")
    else:
        raise Exception("Invalid pattern")

    # Translate input path to absolute path:
    folderpath = os.path.abspath(folderpath)

    # Inspect the given folder for time step files:
    scanfiles = {}
    for filename in gen_all_tiff_of_pathtree(folderpath):
        found = re.search(pat, str(filename))
        if found:
            scanstep = float(found.group(1))
            scanfiles[scanstep] = filename

    # Generate delay axis:
    axlist = []
    for scanstep in iter(sorted(scanfiles.keys())):
        axlist.append(scanstep)
    if pattern == "ch7tr":
        scanvalues = u.to_ureg(np.array(axlist), scanunit)

    scanaxis = snomtools.data.datasets.Axis(scanvalues, label=scanaxislabel, plotlabel=scanaxispl)

    # Test data size:
    if detector == "dld_ch7":
        sample_data = peem_dld_read_ch7(os.path.join(folderpath, scanfiles[list(scanfiles.keys())[0]]))
    else:
        sample_data = tf.peem_camera_read_terra(os.path.join(folderpath, scanfiles[list(scanfiles.keys())[0]]))
    axlist = [scanaxis] + sample_data.axes
    newshape = scanaxis.shape + sample_data.shape

    # Build the data-structure that the loaded data gets filled into
    if h5target:
        compression = 'gzip'
        compression_opts = 4

        # Handle chunking and optimize buffer size:
        if chunks is True:  # Default is auto chunk alignment, so we need to probe.
            max_available_cache = psutil.virtual_memory().available * 0.7  # 70 % of available memory
            if MAX_CACHE_SIZE:
                max_available_cache = min(max_available_cache, MAX_CACHE_SIZE)  # Stay below hardcoded debug limit.
            use_chunk_size = probe_chunksize(shape=newshape, compression=compression, compression_opts=compression_opts)
            use_cache_size = buffer_needed(newshape, (0,), use_chunk_size, dtype=np.float32)
            while use_cache_size > max_available_cache:
                if verbose:
                    print("Warning: Chunk alignment {0} to large for available cache.".format(use_chunk_size))
                # Reduce chunk scan chunk size by half:
                use_chunk_size = (use_chunk_size[0] // 2,) + use_chunk_size[1:]
                # Note: It looks tempting to do something like this to keep overall chunk size constant:
                # ndims = len(use_chunk_size[1:])
                # use_chunk_size = (use_chunk_size[0] // 2,) + tuple(
                #     int(n * (2 ** (1 / ndims))) for n in use_chunk_size[1:])
                # This is NOT better, because of overhang (length % chunksize) which can exceed buffer!
                # I'll not do an advanced search for optimal values for constant size,
                # because the chunk size should still be reasonable down to 1/16 MB or so.
                # MH, 2020-04-16
                if use_chunk_size[0] < 1:  # Avoid edge case 0
                    use_chunk_size = (1,) + use_chunk_size[1:]
                    print("Warning: Cannot reduce chunk size to fit into available buffer. Readin might be slow!")
                    use_cache_size = buffer_needed(newshape, (0,), use_chunk_size, dtype=np.float32)
                    break
                if verbose:
                    print("Using half chunk size along scan direction: {0}".format(use_chunk_size))
                use_cache_size = buffer_needed(newshape, (0,), use_chunk_size, dtype=np.float32)
        elif chunks:  # Chunk size is explicitly set:
            use_chunk_size = chunks
            use_cache_size = buffer_needed(newshape, (0,), use_chunk_size, dtype=np.float32)
        else:  # Chunked storage is turned off:
            use_chunk_size = False
            use_cache_size = None

        # Initialize full DataSet with zeroes:
        dataspace = snomtools.data.datasets.Data_Handler_H5(unit=sample_data.get_datafield(0).get_unit(),
                                                            shape=newshape, chunks=use_chunk_size,
                                                            compression=compression, compression_opts=compression_opts,
                                                            chunk_cache_mem_size=use_cache_size,
                                                            dtype=np.float32)
        dataarray = snomtools.data.datasets.DataArray(dataspace,
                                                      label=sample_data.get_datafield(0).get_label(),
                                                      plotlabel=sample_data.get_datafield(0).get_plotlabel(),
                                                      h5target=dataspace.h5target,
                                                      chunks=use_chunk_size,
                                                      compression=compression, compression_opts=compression_opts,
                                                      chunk_cache_mem_size=use_cache_size)
        dataset = snomtools.data.datasets.DataSet("TR Scan " + folderpath, [dataarray], axlist, h5target=h5target,
                                                  chunk_cache_mem_size=use_cache_size)
    else:
        # In-memory data processing without h5 files.
        dataspace = u.to_ureg(np.zeros(newshape, dtype=np.float32), sample_data.datafields[0].get_unit())
        dataarray = snomtools.data.datasets.DataArray(dataspace,
                                                      label=sample_data.get_datafield(0).get_label(),
                                                      plotlabel=sample_data.get_datafield(0).get_plotlabel(),
                                                      h5target=None)
        dataset = snomtools.data.datasets.DataSet("Terra Scan " + folderpath, [dataarray], axlist, h5target=h5target)

    dataarray = dataset.get_datafield(0)

    # Fill in data from imported tiffs:
    slicebase = tuple([np.s_[:] for j in range(len(sample_data.shape))])

    if verbose:
        import time
        print("Reading Terra Scan Folder of shape: ", dataset.shape)
        if h5target:
            print("... generating chunks of shape: ", dataset.get_datafield(0).data.ds_data.chunks)
            if dataset.own_h5file:
                print("... using cache size {0:d} MB".format(dataset.h5target.get_chunk_cache_mem_size() // 1024 ** 2))
        else:
            print("... in memory")
        start_time = time.time()

    for i, scanstep in zip(list(range(len(scanfiles))), iter(sorted(scanfiles.keys()))):
        islice = (i,) + slicebase
        # Import tiff:
        if detector == "dld":
            idata = tf.peem_dld_read_terra(os.path.join(folderpath, scanfiles[scanstep]))
        elif detector == "dld-sum":
            idata = tf.peem_dld_read_terra_sumimage(os.path.join(folderpath, scanfiles[scanstep]))
        elif detector == "dld_ch7":
            idata = peem_dld_read_ch7(os.path.join(folderpath, scanfiles[scanstep]))
        else:
            idata = tf.peem_camera_read_terra(os.path.join(folderpath, scanfiles[scanstep]))
        # Check data consistency:
        assert idata.shape == sample_data.shape, "Trying to combine scan data with different shape."
        for ax1, ax2 in zip(idata.axes, sample_data.axes):
            assert ax1.units == ax2.units, "Trying to combine scan data with different axis dimensionality."
        assert idata.get_datafield(0).units == sample_data.get_datafield(0).units, \
            "Trying to combine scan data with different data dimensionality."
        # Write data:
        dataarray[islice] = idata.get_datafield(0).data
        if verbose:
            tpf = ((time.time() - start_time) / float(i + 1))
            etr = tpf * (dataset.shape[0] - i + 1)
            print("tiff {0:d} / {1:d}, Time/File {3:.2f}s ETR: {2:.1f}s".format(i, dataset.shape[0], etr, tpf))

    return dataset


    print("done")