"""Imports time-resolved measurements at Chamber 7."""


from __future__ import absolute_import
from __future__ import division
from __future__ import print_function
import snomtools.data.imports.tiff as tf
import snomtools.data.datasets
import os
import numpy as np
import tifffile
import re
import sys
import psutil
import snomtools.calcs.units as u
from pathlib import Path
from snomtools.data.h5tools import probe_chunksize, buffer_needed

if '-v' in sys.argv or __name__ == "__main__":
    verbose = True
else:
    verbose = False

MAX_CACHE_SIZE = None  # 4 * 1024 ** 3  # 4 GB


def strs_in_path(path, strings):
    for string in strings:
        if string in path:
            return True
    return False


def check_min_max(curr_min, curr_max, val):
    if curr_min is None or curr_min > val:
        curr_min = val
    if curr_max is None or curr_max < val:
        curr_max = val
    return curr_min, curr_max


def gen_all_files_of_pathtree(folderpath, filesuffixes, ignores = None):
    """
    Returns a generator that yields the Path to all files with types passed in filesuffixes recursively

    :param str/path folderpath: The folderpath that will be crawled for files of a type

    :param List[str] a list of filesuffixes that are valid. Works only for format ".type" not "type"

    :param List[str] if str occurs in path, path will be ignored.

    :return: Generator for all filepaths found

    """
    if ignores is None:
        ignores = []
    return (Path(path, file) for path, _, files in os.walk(folderpath) if not strs_in_path(path, ignores) for file in
            files if Path(file).suffix in filesuffixes)


def gen_all_tiff_of_pathtree(folderpath):
    """
    Returns a generator that yields the Path to all .tiff/.tif of a path recursively.

    :param str/path folderpath: The folderpath that will be crawled for files of a type

    :return: Generator for all filepaths found

    """
    return gen_all_files_of_pathtree(folderpath, [".tiff", ".tif"])


def gen_all_non_tr_tiff(folderpath):
    """
    Returns a generator that yields the Path to all .tiff/.tif of a path recursively and ignores tr folders

    :param str/path folderpath: The folderpath that will be crawled for files of a type

    :return: Generator for all filepaths found

    """

    return gen_all_files_of_pathtree(folderpath, [".tiff", ".tif"], ignores=["tr", "TR", "Tr"])


def ch7_read_tiff_info(filepath):
    """
    Reads tif info files of ch7 to a dict<Info,Valuelist>

    :param filepath: path of info input file
    :return: dict<Info,Valuelist>
    """
    info_re = [
        "(Information.+File) (.+tif)",
        "(Originally.+in) (.+)",
        "(Date): (\d{4}-\d{2}-\d{2})",
        "(Time): (\d{2}:\d{2})",
        "(binning x): (\d+)",
        "(binning y): (\d+)",
        "(binning t): (\d+)",
        "(region of interest x): (\d+) to (\d+)",
        "(region of interest y): (\d+) to (\d+)",
        "(region of interest t): (\d+) to (\d+)",
        "(modulo): (\d+)"
    ]

    info_dict = {}

    with open(filepath) as file:
        info = file.read()
        for rexp in info_re:
            groups = re.search(re.compile(rexp), info).groups()
            tmp_value_list = []

            for value in groups[1:]:
                tmp_value_list.append(value)
            info_dict[groups[0]] = tmp_value_list

    return info_dict


def ch7_dld_read(filepath, h5target=None):
    """
    Reads a tif file as generated by ch7 when using the DLD. Therefore, the 3D tif dimensions are interpreted as
    time-channel, x and y.

    :param filepath: String: The (absolute or relative) path of input file.

    :return: The dataset instance generated from the tif file.
    """
    # Translate input path to absolute path:
    filepath = os.path.abspath(filepath)
    filebase = os.path.basename(filepath)
    fileparent = os.path.abspath(os.path.join(filepath, os.pardir))

    # Get Metadata from file_info.txt
    infopath = Path(fileparent, Path(filebase).stem + "_info.txt")
    print(infopath)
    meta_dict = ch7_read_tiff_info(infopath)

    T = int(meta_dict["region of interest t"][0])
    Xbin = int(meta_dict["binning x"][0])
    Ybin = int(meta_dict["binning y"][0])
    Tbin = int(meta_dict["binning t"][0])

    # Read tif file to numpy array. Axes will be (timechannel, x, y):
    infile = tifffile.TiffFile(filepath)
    indata = infile.asarray(np.s_[:])
    infile.close()

    # Initialize data for dataset:
    dataarray = snomtools.data.datasets.DataArray(indata, unit='count', label='counts', plotlabel='Counts',
                                                  h5target=h5target)

    # The following commented lines won't work because of Terras irreproducible channel assignment and saving...
    # assert (realdata.shape[0] == round(St / float(Tbin))), \
    # 	"ERROR: Tifffile metadata time binning does not fit to data size."
    # uplim = T+(round(St/float(Tbin)))*Tbin # upper limit calculation
    # So just take the data dimensions... and pray the channels start at the set T value:
    taxis = snomtools.data.datasets.Axis([T + i * Tbin for i in range(indata.shape[0])], label='channel',
                                         plotlabel='Time Channel')


    # Careful about orientation! This is like a matrix:
    # rows go first and are numbered in vertical direction -> Y
    # columns go last and are numbered in horizontal direction -> X
    yaxis = snomtools.data.datasets.Axis([i * Xbin for i in range(indata.shape[1])],
                                         unit='pixel', label='y', plotlabel='y')
    xaxis = snomtools.data.datasets.Axis([i * Xbin for i in range(indata.shape[2])],
                                         unit='pixel', label='x', plotlabel='x')

    # Return dataset:
    return snomtools.data.datasets.DataSet(label=filebase, datafields=[dataarray], axes=[taxis, yaxis, xaxis])


def ch7_dld_read_all_static(folderpath):
    """
    Greedy function that converts all tiffs anywhere in the folderpath (subdirs too!) use with care! Get a coffee!
    Can produce chaos and despair if it breaks midways.
    IGNORES FOLDERS WITH "tr","Tr","TR" IN THEIR PATH! If that happens you probably want to use measurement_folder_peem

    :param folderpath: Folderpath in which tiff are searched and converted

    :return: None
    """

    for filepath in gen_all_non_tr_tiff(folderpath):
        dest = Path(filepath.parent) / filepath.with_suffix('hdf5')
        print(dest)
        dataset = ch7_dld_read(filepath)
        dataset.saveh5(h5dest=str(dest))


def ch7_dld_read_all_static_to_dest_with_structure(folderpath, destination):
    """
    Greedy function that converts all tiffs anywhere in the folderpath (subdirs too!) use with care! Get a coffee!
    Can produce chaos and despair if it breaks midways.
    IGNORES FOLDERS WITH "tr","Tr","TR" IN THEIR PATH! If that happens you probably want to use measurement_folder_peem
    This alteration of ch7_dld_read_all_static reconstructs the directory structure relative to folderpath at
    destination path. For example with folderpath='C:/ch7/measurements', 'destination='C:/ch7/hdf5'
    he might read a 'C:/ch7/measurements/measurementx/measurementx_1.tiff'
    to 'C:/ch7/hdf5/measurementx/measuremenx_1.hdf5'

    :param folderpath: Folderpath in which tiff are searched and converted
    :param destination: Folderpath to which the directory structure will be built and hdf5 written as leaf

    :return: None
    """
    destination = Path(destination)
    for filepath in gen_all_non_tr_tiff(folderpath):
        tmpdest = destination / (Path(filepath.parent).relative_to(Path(folderpath))) / filepath.with_suffix('hdf5')
        Path.mkdir(destination, parents=True, exist_ok=True)
        print(f'Saving to: {tmpdest}')
        ch7_dld_read(filepath, h5target=tmpdest)


def ch7_read_tr_folder(folderpath, detector="dld_ch7", pattern="ch7tr", scanunit="fs",
                       scanaxislabel="delay", scanaxispl=None, h5target=True, chunks=True):
    """
    Method to read Chamber 7's time resolved measurements. Will dig for tiff files, so ensure that online one
    measurement is contained in folderpath. Will try to isolate cycles (In the most cruel way, may it bring as much pain
    while reading as it did while writing)

    :param str folderpath: The path of the folder containing the scan data. Example: "/path/to/measurement/1. Durchlauf"

    :param str detector: Read mode corresponding to the used detector.
        Valid inputs:
            * :code:`"dld_ch7"` for reading ch7 dld data.

    :param str pattern: Kept only for consistent call signature with `data.imports.tiff` Methods. Only valid option:
            * :code:`"ch7tr"' for ch7 pattern

    :param str scanunit: A valid unit string, according to the physical dimension that was scanned over.

    :param str scanaxislabel: A label for the axis of the scan.

    :param str scanaxispl: A plot label for the axis of the scan.

    :param h5target: The HDF5 target to write to.
    :type h5target: str **or** h5py.Group **or** True (the default), *optional*

    .. warning::
        The default value `True` for `h5target` reads into a DataSet in temp file mode.
        The cache size (`chunk_cache_mem_size`) is not forwarded to DataArrays in this DataSet.
        Therefore the data is read in with default cache size, which can be **very** slow!

        Use the proper target for `h5target` directly for maximum performance.

    :param chunks: If given as a tuple of ints of length of the to-be-read data dimensionality, this can be used to
        explicitly set the chunk alignment for the output data. Useful if a specific chunk size is desired.
        If `True` is given (the default), the chunk alignment is chosen automatically.
        Ignored if h5target is `False`.
        Chunking can be turned off by giving `False` (not recommended).
    :type chunks: tuple *of* ints **or** bool

    :return: List of Imported DataSets.
    :rtype: List[DataSet]

    """
    assert detector in ["dld_ch7"], "Invalid detector mode."
    if scanaxispl is None:
        scanaxispl = 'Scan / ' + scanunit

    # Compile regex for file detection:

    if pattern == "ch7tr":
        pat = re.compile("(\d{3})_(\d{4})_(.+?)_.+tif")
    else:
        raise Exception("Invalid pattern")

    # Translate input path to absolute path:
    folderpath = os.path.abspath(folderpath)

    # Inspect the given folder for time step files and check for max and min scanstep to slice runs:
    scannedfiles = {}
    scanstep_max = None
    scanstep_min = None

    for filename in gen_all_tiff_of_pathtree(folderpath):
        found = re.search(pat, str(filename))
        if found:
            scanindex = int(found.group(2))
            scanstep = float(found.group(3))
            scanstep_min, scanstep_max = check_min_max(scanstep_min, scanstep_max, scanstep)
            scannedfiles[scanindex] = {'filename': filename, 'scanstep': scanstep}

    # Check for multiple runs and split them
    min_or_max_index = [i for i in range(len(scannedfiles))
                        if scannedfiles[i]['scanstep'] == scanstep_min or scannedfiles[i]['scanstep'] == scanstep_max]

    if min_or_max_index[0] != 0:
        min_or_max_index.insert(0, 0)
    if min_or_max_index[-1] != len(scannedfiles) - 1:
        min_or_max_index.insert(-1, len(scannedfiles) - 1)

    scanstep_filename_dicts_list = [{scannedfiles[i]['scanstep']:scannedfiles[i]['filename']
                      for i in range(min_or_max_index[m], min_or_max_index[m+1], 1)}
                      for m in range(0, len(min_or_max_index)-1, 2)]

    print(min_or_max_index)

    #Create Datasets by iteration over runs/dicts in scanstep_filename_dicts_list
    dataset_list = []

    for n, cycle in enumerate(scanstep_filename_dicts_list):
        # Generate delay axis:
        axlist = []
        for scanstep in iter(sorted(cycle.keys())):
            axlist.append(scanstep)
        scanvalues = u.to_ureg(np.array(axlist), scanunit)

        scanaxis = snomtools.data.datasets.Axis(scanvalues, label=scanaxislabel, plotlabel=scanaxispl)

        # Test data size:
        if detector == "dld_ch7":
            sample_data = ch7_dld_read(os.path.join(folderpath, cycle[list(cycle.keys())[0]]))
        else:
            raise NotImplementedError("Invalid Detector Mode.")
        axlist = [scanaxis] + sample_data.axes
        newshape = scanaxis.shape + sample_data.shape

        # Build the data-structure that the loaded data gets filled into
        if h5target:
            compression = 'gzip'
            compression_opts = 4

            # Handle chunking and optimize buffer size:
            if chunks is True:  # Default is auto chunk alignment, so we need to probe.
                max_available_cache = psutil.virtual_memory().available * 0.7  # 70 % of available memory
                if MAX_CACHE_SIZE:
                    max_available_cache = min(max_available_cache, MAX_CACHE_SIZE)  # Stay below hardcoded debug limit.
                use_chunk_size = probe_chunksize(shape=newshape, compression=compression, compression_opts=compression_opts)
                use_cache_size = buffer_needed(newshape, (0,), use_chunk_size, dtype=np.float32)
                while use_cache_size > max_available_cache:
                    if verbose:
                        print("Warning: Chunk alignment {0} to large for available cache.".format(use_chunk_size))
                    # Reduce chunk scan chunk size by half:
                    use_chunk_size = (use_chunk_size[0] // 2,) + use_chunk_size[1:]
                    # Note: It looks tempting to do something like this to keep overall chunk size constant:
                    # ndims = len(use_chunk_size[1:])
                    # use_chunk_size = (use_chunk_size[0] // 2,) + tuple(
                    #     int(n * (2 ** (1 / ndims))) for n in use_chunk_size[1:])
                    # This is NOT better, because of overhang (length % chunksize) which can exceed buffer!
                    # I'll not do an advanced search for optimal values for constant size,
                    # because the chunk size should still be reasonable down to 1/16 MB or so.
                    # MH, 2020-04-16
                    if use_chunk_size[0] < 1:  # Avoid edge case 0
                        use_chunk_size = (1,) + use_chunk_size[1:]
                        print("Warning: Cannot reduce chunk size to fit into available buffer. Readin might be slow!")
                        use_cache_size = buffer_needed(newshape, (0,), use_chunk_size, dtype=np.float32)
                        break
                    if verbose:
                        print("Using half chunk size along scan direction: {0}".format(use_chunk_size))
                    use_cache_size = buffer_needed(newshape, (0,), use_chunk_size, dtype=np.float32)
            elif chunks:  # Chunk size is explicitly set:
                use_chunk_size = chunks
                use_cache_size = buffer_needed(newshape, (0,), use_chunk_size, dtype=np.float32)
            else:  # Chunked storage is turned off:
                use_chunk_size = False
                use_cache_size = None

            # Initialize full DataSet with zeroes:
            dataspace = snomtools.data.datasets.Data_Handler_H5(unit=sample_data.get_datafield(0).get_unit(),
                                                                shape=newshape, chunks=use_chunk_size,
                                                                compression=compression, compression_opts=compression_opts,
                                                                chunk_cache_mem_size=use_cache_size,
                                                                dtype=np.float32)
            dataarray = snomtools.data.datasets.DataArray(dataspace,
                                                          label=sample_data.get_datafield(0).get_label(),
                                                          plotlabel=sample_data.get_datafield(0).get_plotlabel(),
                                                          h5target=dataspace.h5target,
                                                          chunks=use_chunk_size,
                                                          compression=compression, compression_opts=compression_opts,
                                                          chunk_cache_mem_size=use_cache_size)
            dataset = snomtools.data.datasets.DataSet("TR Scan " + folderpath, [dataarray], axlist, h5target=h5target,
                                                      chunk_cache_mem_size=use_cache_size)
            dataset_list.append(dataset)

        else:
            # In-memory data processing without h5 files.
            dataspace = u.to_ureg(np.zeros(newshape, dtype=np.float32), sample_data.datafields[0].get_unit())
            dataarray = snomtools.data.datasets.DataArray(dataspace,
                                                          label=sample_data.get_datafield(0).get_label(),
                                                          plotlabel=sample_data.get_datafield(0).get_plotlabel(),
                                                          h5target=None)
            dataset = snomtools.data.datasets.DataSet("Terra Scan " + folderpath, [dataarray], axlist, h5target=h5target)
            dataset_list.append(dataset)

        dataarray = dataset.get_datafield(0)

        # Fill in data from imported tiffs:
        slicebase = tuple([np.s_[:] for j in range(len(sample_data.shape))])

        if verbose:
            import time
            print("Reading Terra Scan Folder of shape: ", dataset.shape)
            if h5target:
                print("... generating chunks of shape: ", dataset.get_datafield(0).data.ds_data.chunks)
                if dataset.own_h5file:
                    print("... using cache size {0:d} MB".format(dataset.h5target.get_chunk_cache_mem_size() // 1024 ** 2))
            else:
                print("... in memory")
            start_time = time.time()

        for i, scanstep in zip(list(range(len(cycle))), iter(sorted(cycle.keys()))):
            islice = (i,) + slicebase
            # Import tiff:
            if detector == "dld_ch7":
                idata = ch7_dld_read(os.path.join(folderpath, cycle[scanstep]))
            else:
                raise NotImplementedError("Invalid Detector Mode.")
            # Check data consistency:
            assert idata.shape == sample_data.shape, "Trying to combine scan data with different shape."
            for ax1, ax2 in zip(idata.axes, sample_data.axes):
                assert ax1.units == ax2.units, "Trying to combine scan data with different axis dimensionality."
            assert idata.get_datafield(0).units == sample_data.get_datafield(0).units, \
                "Trying to combine scan data with different data dimensionality."
            # Write data:
            dataarray[islice] = idata.get_datafield(0).data
            if verbose:
                tpf = ((time.time() - start_time) / float(i + 1))
                etr = tpf * (dataset.shape[0] - i + 1)
                print("tiff {0:d} / {1:d}, Time/File {3:.2f}s ETR: {2:.1f}s".format(i, dataset.shape[0], etr, tpf))

    return dataset_list

if __name__ == "__main__":
    tr_test_path = r"Z:\PEEM_c7\2021\20210417_Au111_1h50minCo\time-resolved"
    h5target = r"E:\Uni\Aeschliwi\test_tr_multiple_cycle.hdf5"

    datasets = ch7_read_tr_folder(tr_test_path)

    for n,dataset in enumerate(datasets):
        h5target = Path(h5target)
        h5target = str(h5target.with_stem(h5target.stem + str(n)))
        dataset.saveh5(h5dest=h5target)

    print("done")